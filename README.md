# Drive-WM
## Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving

[Yuqi Wang*](https://robertwyq.github.io/), [Jiawei He*](https://jiaweihe.com/), [Lue Fan*](https://lue.fan/), [Hongxin Li*](https://github.com/ZJULiHongxin), [Yuntao Chenâ€ ](https://scholar.google.com/citations?user=iLOoUqIAAAAJ), [Zhaoxiang Zhangâ€ ](https://zhaoxiangzhang.net/)
(*: Equal Contribution; â€ : Corresponding Author)

[[arXiv](https://arxiv.org/abs/2311.17918)] [[Project page](https://drive-wm.github.io/)]

![logo](https://github.com/BraveGroup/Drive-WM/assets/27729041/8418123e-35fa-450a-abd6-da266461cb78)

## TO DO
- [ ] Conditional image generation model weights (coming soonðŸš€)
- [ ] Conditional video generation model weights
- [ ] Action-conditioned video forecast model weights
- [ ] Training codes for all models

## Get Started
Our code is based on the open-source project diffusers.

## Citation
```
@article{wang2023driving,
  title={Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving},
  author={Wang, Yuqi and He, Jiawei and Fan, Lue and Li, Hongxin and Chen, Yuntao and Zhang, Zhaoxiang},
  journal={arXiv preprint arXiv:2311.17918},
  year={2023}
}
```

## Acknowledgement 
Many thanks to the following open-source projects:
* [diffusers](https://github.com/huggingface/diffusers)

